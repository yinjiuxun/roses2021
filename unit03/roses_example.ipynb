{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704fb130",
   "metadata": {},
   "source": [
    "<img src=\"figures/header.png\" width=\"800\" height=\"200\">\n",
    "\n",
    "# Lecture 3: Seismic Tomography\n",
    "\n",
    "Notebook Authors: Chet Goerzen & Adebayo Ojo\n",
    "\n",
    "# Introduction\n",
    "In this notebook, we will walk through the steps of Ambient Noise Tomography. The first step will be extracting the Green's functions from ambient seismic noise. The next step will be measuring the dispersion curve from the extracted Green's function. Then, we will use a previously created dataset to perform a 2-D tomographic inversion, and finish with a synthetic test of the tomographic inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6eef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import obspy\n",
    "import pyasdf # For accessing the data format used by NoisePy\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import UTCDateTime\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy # Mainly used for interpolation\n",
    "import pycwt # We will need to do this to perform the wavelet analysis for dispersion curve measurement\n",
    "from obspy.clients.fdsn import Client\n",
    "\n",
    "client = Client(\"IRIS\") # Initialize Client to download data from IRIS\n",
    "\n",
    "\n",
    "import noise_module # Import code from NoisePy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8cb9e",
   "metadata": {},
   "source": [
    "# Disclaimer:\n",
    "\n",
    "While derived from previous examples, the code in this notebook has not been extensively tested. If you would like to use this code for your own research, it would be wise to critically analyze the code and perform your own tests.\n",
    "\n",
    "# Extraction of Green's Functions from Ambient Noise\n",
    "\n",
    "This section is based on the example provided with the NoisePy code (https://github.com/chengxinjiang/NoisePy/blob/master/Jupyter_notebook/download_toASDF_cross_correlation.ipynb). The original notebook was produced by Dr. Chengxin Jiang\n",
    "\n",
    "The following cell downloads data from two stations of the Transportable Array and does some pre-processing. The trend and mean of the data are removed, gaps are filled with zeros, the sampling frequency is downsampled from 40 Hz to 3 Hz, and the instrument response is removed. In this case I've already downloaded the data, but one may use this to download data of your own. In this case only 5 days of data is used, which is a very short period for ambient noise tomography. In order to get good results one may need many months or even years of data. NoisePy offers scripts to either download data or re-organize an existing dataset on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46270fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from NoisePy\n",
    "\n",
    "# samp_freq = 3                      # targeted sampling rate at X samples per seconds \n",
    "# rm_resp   = 'inv'                  # select 'no' to not remove response and use 'inv','spectrum','RESP', or 'polozeros' to remove response\n",
    "# respdir   = './'                   # directory where resp files are located (required if rm_resp is neither 'no' nor 'inv')\n",
    "# freqmin   = 0.25                   # pre filtering frequency bandwidth\n",
    "# freqmax   = 1                      # note this cannot exceed Nquist freq                         \n",
    "\n",
    "# chan = ['BHZ','BHZ']               # channel for each station\n",
    "# net  = ['TA','TA']                 # network for each station \n",
    "# sta  = ['K62A','K63A']             # station (using a station list is way either compared to specifying stations one by one)\n",
    "# start_date = [\"2014_01_01_0_0_0\"]  # start date of download\n",
    "# end_date   = [\"2014_01_05_0_0_0\"]  # end date of download\n",
    "# inc_hours  = 24                    # length of data for each request (in hour)\n",
    "# nsta       = len(sta)\n",
    "\n",
    "# # save prepro parameters into a dict\n",
    "# prepro_para = {'rm_resp':rm_resp,'respdir':respdir,'freqmin':freqmin,'freqmax':freqmax,'samp_freq':samp_freq,'start_date':\\\n",
    "#     start_date,'end_date':end_date,'inc_hours':inc_hours}\n",
    "\n",
    "# # convert time info to UTC format\n",
    "# starttime = obspy.UTCDateTime(start_date[0])       \n",
    "# endtime   = obspy.UTCDateTime(end_date[0])\n",
    "# date_info = {'starttime':starttime,'endtime':endtime}\n",
    "\n",
    "# # write into ASDF file: using start and end time as file name\n",
    "# ff=os.path.join('roses_test.h5')\n",
    "# with pyasdf.ASDFDataSet(ff,mpi=False,compression=\"gzip-3\",mode='w') as ds:\n",
    "\n",
    "#     # loop through each station\n",
    "#     for ista in range(nsta):\n",
    "\n",
    "#         # get inventory for each station\n",
    "#         try:\n",
    "#             sta_inv = client.get_stations(network=net[ista],station=sta[ista],\\\n",
    "#                 location='*',starttime=starttime,endtime=endtime,level=\"response\")\n",
    "#         except Exception as e:\n",
    "#             print(e);continue\n",
    "\n",
    "#         # add the inventory into ASDF        \n",
    "#         try:\n",
    "#             ds.add_stationxml(sta_inv) \n",
    "#         except Exception: \n",
    "#             pass   \n",
    "\n",
    "#         try:\n",
    "#             # get data\n",
    "#             tr = client.get_waveforms(network=net[ista],station=sta[ista],\\\n",
    "#                 channel=chan[ista],location='*',starttime=starttime,endtime=endtime)\n",
    "#         except Exception as e:\n",
    "#             print(e,'for',sta[ista]);continue\n",
    "            \n",
    "#         # preprocess to clean data  \n",
    "#         print('working on station '+sta[ista])\n",
    "#         tr = noise_module.preprocess_raw(tr,sta_inv,prepro_para,date_info)\n",
    "\n",
    "#         if len(tr):\n",
    "#             new_tags = '{0:s}_00'.format(chan[ista].lower())\n",
    "#             ds.add_waveforms(tr,tag=new_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc16d64f",
   "metadata": {},
   "source": [
    "In this next cell, all the parameters needed to set up the cross-correlation are defined. In this case, the recorded time series will be broken into thirty minute segments. The step size is 15 minutes, leading to an overlap of 50%. In this case, both frequency and time domain normalization is used. The time domain normalization used in this example is one-bit normalization. This means that only the sign of the time series recorded at each station is saved. This is done to reduce the effect of transient signals such as earthquakes. A running mean average is applied to the amplitude spectrum of the signal, with a window length of 100 samples. This smooths the frequency content of the signal. The data will also be bandpass filtered between 0.25 Hz and 1 Hz, as this is the frequency range we are interested in for this example. The maximum time lag to be saved is 200 seconds, meaning that the cross-correlation will be saved from -200 to 200 seconds. Any signals greater than 10 times the standard deviation of the normalized and filtered signal will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00212ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from NoisePy\n",
    "\n",
    "# Here we will load the raw waveform, that has previosuly been downloaded.\n",
    "sfile = glob.glob('data/ROSES_WAVEFORM.h5')      # find hdf5 files\n",
    "\n",
    "# Set up the parameters\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "# parameters of fft_cc\n",
    "cc_len    = 1800         # window length (sec) to cut daily data into small segments\n",
    "step      = 900          # overlapping (sec) between the sliding window\n",
    "smooth_N  = 100          # number of points to be smoothed for running-mean average (time-domain)\n",
    "inc_hours = 24           # basic length (hour) of the continous noise data        \n",
    "\n",
    "freq_norm   = 'rma'      # rma-> running mean average for frequency-domain normalization\n",
    "time_norm   = 'one_bit'  # no-> no time-domain normalization; other options 'rma' for running-mean and 'one_bit'\n",
    "cc_method   = 'xcorr'    # xcorr-> pure cross correlation; other option is 'decon'\n",
    "substack       = False   # sub-stack daily cross-correlation or not, doesn't matter for this example\n",
    "substack_len   = cc_len  # how long to stack over: need to be multiples of cc_len,\n",
    "                         # changing this will not affect this example\n",
    "    \n",
    "smoothspect_N  = 100     # number of points to be smoothed for running-mean average (freq-domain)\n",
    "freqmin   = 0.25         # pre filtering frequency bandwidth\n",
    "freqmax   = 1            # note this cannot exceed Nquist freq \n",
    "\n",
    "# cross-correlation parameters\n",
    "maxlag       = 200       # time lag (sec) for the cross correlation functions\n",
    "max_over_std = 10        # amplitude threshold to remove segments of spurious phases\n",
    "# --------------------------------------------------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8253769",
   "metadata": {},
   "source": [
    "Now we'll load in the data for both the receiver and source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from NoisePy\n",
    "\n",
    "# Load the raw waveform to memory, along with some metadata info\n",
    "# -------------------------------------------------------------------- #\n",
    "with pyasdf.ASDFDataSet(sfile[0],mode='r') as ds:\n",
    "    \n",
    "    # station list\n",
    "    sta_list = ds.waveforms.list()\n",
    "    print('source of %s and receiver %s'%(sta_list[0],sta_list[1]))\n",
    "    \n",
    "    # channels for each station\n",
    "    all_tags = ds.waveforms[sta_list[0]].get_waveform_tags()\n",
    "    \n",
    "    # get the source trace\n",
    "    tr_source = ds.waveforms[sta_list[0]][all_tags[0]]\n",
    "    \n",
    "    # get the source trace\n",
    "    tr_receiver = ds.waveforms[sta_list[1]][all_tags[0]]\n",
    "    \n",
    "    # read inventory\n",
    "    inv1 = ds.waveforms[sta_list[1]]['StationXML']\n",
    "    \n",
    "    # read receiver station info from inventory\n",
    "    rsta,rnet,rlon,rlat,elv,loc = noise_module.sta_info_from_inv(inv1)\n",
    "    \n",
    "    # Get the sampling information\n",
    "    dt = tr_source[0].stats.delta\n",
    "    samp_freq = 1 /dt\n",
    "    \n",
    "    # read inventory\n",
    "    inv1 = ds.waveforms[sta_list[0]]['StationXML']\n",
    "    \n",
    "    # read source station info from inventory\n",
    "    ssta,snet,slon,slat,elv,loc = noise_module.sta_info_from_inv(inv1)\n",
    "# -------------------------------------------------------------------- #\n",
    "    \n",
    "# group parameters into a dict\n",
    "fc_para={'samp_freq':samp_freq,'dt':dt,'cc_len':cc_len,'step':step,'freq_norm':freq_norm,'time_norm':time_norm,\\\n",
    "    'cc_method':cc_method,'maxlag':maxlag,'max_over_std':max_over_std,'inc_hours':inc_hours,'smooth_N':smooth_N,\\\n",
    "    'freqmin':freqmin,'freqmax':freqmax,'smoothspect_N':smoothspect_N,'substack':substack,\\\n",
    "    'substack_len':substack_len}\n",
    "\n",
    "# Calculate distance between source and receiver\n",
    "dist,_,_ = obspy.geodetics.base.gps2dist_azimuth(slat,slon,rlat,rlon)\n",
    "\n",
    "# cut source traces into small segments and make statistics\n",
    "trace_stdS,dataS_t,dataS = noise_module.cut_trace_make_stat(fc_para,tr_source)\n",
    "\n",
    "# do fft to freq-domain\n",
    "source_white = noise_module.noise_processing(fc_para,dataS)\n",
    "source_white = np.conjugate(source_white) # Take the conjugate, so that the source spectrum is consistent with\n",
    "                                          # the receiver function\n",
    "\n",
    "# cut receiver traces into small segments and make statistics\n",
    "trace_stdR,dataR_t,dataR = noise_module.cut_trace_make_stat(fc_para,tr_receiver)\n",
    "\n",
    "# do fft to freq-domain\n",
    "receiver_white = noise_module.noise_processing(fc_para,dataR)\n",
    "\n",
    "# num of frequency data\n",
    "Nfft = source_white.shape[1]\n",
    "Nfft2 = Nfft//2\n",
    "\n",
    "# find the right index of good signals for source and receiver\n",
    "sou_ind = np.where((trace_stdS<max_over_std)&(trace_stdS>0)&(np.isnan(trace_stdS)==0))[0]\n",
    "rec_ind = np.where((trace_stdR<max_over_std)&(trace_stdR>0)&(np.isnan(trace_stdR)==0))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b873d6",
   "metadata": {},
   "source": [
    "The below plot shows the pre-processed time series. Note that the frequency normalization has changed the amplitude of the signal. Without frequency normalization the only values would be 1 or -1, due to the one-bit normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the processing scheme employed by NoisePy, the pre-processed data is stored in the frequency domain.\n",
    "# In order to see the time domain the inverse fourier transform must be applied\n",
    "time_series = np.fft.ifft(source_white[1])\n",
    "\n",
    "# Some small complex values are left after the inverse fourier transform. Using np.real() will discard them.\n",
    "real_time_series = np.real(time_series)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(real_time_series)\n",
    "ax.set_title(\"One segment of pre-processed data from the source station\")\n",
    "ax.set_xlabel(\"Sample Number\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55738f01",
   "metadata": {},
   "source": [
    "Below is the amplitude spectrum. Note that it has been smoothed with a smoothing window of 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cf681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_stack = np.mean(source_white, axis=0) # Stack each spectrum. This is essentially Welch's method\n",
    "freqs = np.fft.rfftfreq(len(fft_stack)-1, dt) # Get the correct frequencies\n",
    "freqs = freqs[1:] # Throw out the DC component\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(freqs, np.abs(fft_stack[1:Nfft2]) ** 2) # Plot the amplitude squared spectrum\n",
    "ax.set_title(\"Amplitude squared spectrum\")\n",
    "ax.set_xlabel(\"Frequency [Hz]\")\n",
    "ax.set_ylabel(\"Amplitude^2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1059ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from NoisePy\n",
    "\n",
    "# find the segments of good data for both source and receiver\n",
    "bb=np.intersect1d(sou_ind,rec_ind)\n",
    "\n",
    "# do cross correlation\n",
    "tdata,t_corr,n_corr = noise_module.correlate(source_white[bb,:Nfft2],receiver_white[bb,:Nfft2],fc_para,Nfft,dataS_t)\n",
    "\n",
    "# plot the waveform\n",
    "tvec = np.linspace(-maxlag,maxlag+dt,len(tdata))\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.plot(tvec,tdata)\n",
    "ax.set_xlabel(\"Time [s]\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "ax.set_title('Cross-Correlation Function Between %s and %s'%(ssta,rsta))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c385197",
   "metadata": {},
   "source": [
    "Notice how asymmetrical the cross-correlation function is!!! It is also pretty noisy. **Question:** What do you think we could do to improve the cross-correlation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8550fbe3",
   "metadata": {},
   "source": [
    "# Measuring Dispersion Curves From Cross-Correlation Functions\n",
    "\n",
    "The following example makes use of code adapted from the dispersion analysis code in NoisePy (https://github.com/chengxinjiang/NoisePy/tree/master/src/application_modules). A frequency-time image is produced using the wavelet transform, and then converted into a velocity-period image. The group velocity curve is then selected from the ridge on the velocity-period image. In this example, we will take the mean of both the positive and negative lags of the cross-correlation function. This is known as the symmetrical component of the cross-correlation function. The method outlined in Jiang & Denolle, 2020 is used to produce the period-velocity image and measure the dispersion curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from NoisePy\n",
    "\n",
    "# Set up the parameters\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "# targeted freq bands for dispersion analysis\n",
    "pmin = 1\n",
    "pmax = 6\n",
    "fmin = 1 / pmax\n",
    "fmax = 1 / pmin\n",
    "per  = np.arange(pmin,pmax,0.02)\n",
    "\n",
    "# set time window for disperion analysis\n",
    "vmin = 2.0\n",
    "vmax = 4.5\n",
    "vel  = np.arange(vmin,vmax,0.02)\n",
    "\n",
    "# basic parameters for wavelet transform, these generally don't need to be changed\n",
    "dj=1/12\n",
    "s0=-1\n",
    "J=-1\n",
    "wvn='morlet'\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "# Calculate the symmetrical waveform\n",
    "npts = int(1/dt) * 2 * maxlag + 1\n",
    "indx = npts // 2\n",
    "data = 0.5 * tdata[indx:] + 0.5 * np.flip(tdata[:indx+1], axis=0)\n",
    "# data = tdata[indx:]\n",
    "\n",
    "dist_km = dist / 1000\n",
    "\n",
    "pt1 = int(dist_km/vmax/dt)\n",
    "pt2 = int(dist_km/vmin/dt)\n",
    "indx = np.arange(pt1, pt2)\n",
    "tvec = indx * dt\n",
    "data = data[indx]\n",
    "\n",
    "# wavelet transformation\n",
    "cwt, sj, freq, coi, _, _ = pycwt.cwt(data, dt, dj, s0, J, wvn)\n",
    "\n",
    "# do filtering here\n",
    "if (fmax > np.max(freq)) | (fmax <= fmin):\n",
    "    raise ValueError('Abort: frequency out of limits!')\n",
    "freq_ind = np.where((freq >= fmin) & (freq <= fmax))[0]\n",
    "cwt = cwt[freq_ind]\n",
    "freq = freq[freq_ind]\n",
    "\n",
    "# use amplitude of the cwt\n",
    "period = 1 / freq\n",
    "rcwt = np.abs(cwt) ** 2\n",
    "\n",
    "# interpolation to grids of freq-vel\n",
    "fc = scipy.interpolate.interp2d(dist_km / tvec, period, rcwt)\n",
    "rcwt_new = fc(vel, per)\n",
    "\n",
    "# do normalization for each frequency\n",
    "for ii in range(len(per)):\n",
    "    rcwt_new[ii] /= np.max(rcwt_new[ii])\n",
    "    \n",
    "nper, gv = noise_module.extract_dispersion(rcwt_new, per, vel) # Measure the dispersion curve from the \n",
    "                                                               # velocity-period map\n",
    "\n",
    "# Find the first instance where the wavelength is greater than 1/3 of the inter-station distance\n",
    "# -------------------------------------- #\n",
    "lmbda = gv * nper\n",
    "bad_idx = np.where(lmbda > dist_km / 3)[0]\n",
    "# -------------------------------------- #\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "im = ax.imshow(np.transpose(rcwt_new),cmap='gnuplot2',extent=[per[0],per[-1],vel[0],vel[-1]],\n",
    "           aspect='auto',origin='lower')\n",
    "\n",
    "ax.plot(nper,gv,'b--', linewidth=4.0)\n",
    "ax.plot([nper[bad_idx[0]], nper[bad_idx[0]]], [vel[0], vel[-1]],\n",
    "        \"-\", color=\"black\", linewidth=4.0) # Plot the first period where lambda is greater\n",
    "                                           # than the distance divided by 3\n",
    "ax.set_xlabel('Period [s]')\n",
    "ax.set_ylabel('U [km/s]')\n",
    "ax.set_title(\"Wavelet Extracted Dispersion Curve\")\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label(\"Normalized Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ce944",
   "metadata": {},
   "source": [
    "This isn't the cleanest dispersion curve, but one can see that there is an obvious moveout with period. \n",
    "\n",
    "**TASK:** Now that you know how the process works, try making a dispersion curve between two stations of your choosing after the lab!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8099c0a",
   "metadata": {},
   "source": [
    "# Making a 2-D map of surface wave velocities\n",
    "\n",
    "There are two further steps to go from a set of dispersion curves to a 3-D map of the subsurface. The first step is to make a 2-D period map. When we measure a dispersion curve between two stations. We are observing the effects of the subsurface over the entire distance along these two stations. The purpose of the 2-D period map is to determine where along this path the variations in velocity occur. Here I'll use seismic-noise-tomography, produced by Dr. Bruno Gotourbes as the framework for developing these 2-D period maps. I'll go through the tomography workflow step by step, although I have also included a wrapper function called `invert4model` in rosesutils that can be used to do the same thing but more concisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ceb8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import time\n",
    "import pygmt\n",
    "import xarray as xr\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import pysismo\n",
    "from pysismo import psutils\n",
    "from pysismo import rosesutils\n",
    "from pysismo.pstomo import Grid\n",
    "\n",
    "# Import utilities for ROSES course\n",
    "# -------------------------------------------- #\n",
    "from pysismo.rosesutils import invert4model # A function to wrap the inversion\n",
    "from pysismo.rosesutils import make_paths # Calculates stright raypaths between stations\n",
    "from pysismo.rosesutils import make_grid # Makes the inversion grid\n",
    "from pysismo.rosesutils import plot_interpolated # Plots an interpolated model overlain on a geographic basemap\n",
    "# -------------------------------------------- #\n",
    "\n",
    "# A notice pops up about reading the configuration file. It is required for the seismic-noise-tomography code\n",
    "# but none of the configuration parameters from this file are used in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d868e805",
   "metadata": {},
   "source": [
    "While it would be nice to calculate our own dispersion curves and use them for tomography, that would simply just take too long. So instead we'll use an existing dataset. Note that in the data directory there is a folder called `cam`. This folder contains data from Ojo, Ni & Li, (2017) and corresponds to a seismic array deployed near Cameroon. The files are organized by period, and contain group velocity measurements for many inter-station paths at the specified period. The data do not have headers, but the column names are added in the following cell, which should make the data contained in each column clear.\n",
    "\n",
    "Here we'll load in the data. Files are named based on the period at which they are measured. In order to clean the data a bit, I'll reject any measurements that fall outside 3 times the standard deviation of the original dataset. There may be better ways to do this, but it is a simple way of cleaning the data and works for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47972297",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_curves = pd.read_csv(\"data/cam/10.dat\", index_col=None, header=None, delim_whitespace=True)\n",
    "disp_curves.columns = [\"source_name\", \"receiver_name\", \"group_velocity\", \"phase_velocity\",\n",
    "                       \"signal2noise_ratio\", \"source_lon\", \"source_lat\", \"receiver_lon\", \"receiver_lat\",\n",
    "                       \"inter-station_distance\"]\n",
    "\n",
    "v_type = \"phase_velocity\" # Use phase velocity for the inversion\n",
    "\n",
    "ori = len(disp_curves)\n",
    "\n",
    "# Remove all measurements greater than 3 times the standard deviation of all phase_velocity measurements\n",
    "# ---------------------------------------------------------------------------------------------------- #\n",
    "std_crit = (np.abs(disp_curves[v_type] - np.mean(disp_curves[v_type]))\\\n",
    "            < 3 * np.std(disp_curves[v_type]))\n",
    "\n",
    "disp_curves = disp_curves[std_crit]\n",
    "# ---------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "clean = len(disp_curves)\n",
    "print(f\"{ori - clean} measurements were removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82601f",
   "metadata": {},
   "source": [
    "Here we will setup the inversion grid. A small buffer of size `tol` is added, to ensure that no raypaths fall outside the grid, which would be bad. A `Grid` object from the seismic-noise-tomography code will be used to handle this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a798d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from seismic-noise-tomgraphy\n",
    "\n",
    "# Setup the inversion grid\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "tol = 0.5\n",
    "lonstep = 0.25\n",
    "latstep = 0.25\n",
    "\n",
    "# Get the smallest longitude\n",
    "# ---------------------------------------------- #\n",
    "min_rcv_lon = np.min(disp_curves[\"receiver_lon\"])\n",
    "min_src_lon = np.min(disp_curves[\"source_lon\"])\n",
    "min_lon = np.min([min_rcv_lon, min_src_lon]) - tol\n",
    "# ---------------------------------------------- #\n",
    "\n",
    "# Get the smallest latitude\n",
    "# ---------------------------------------------- #\n",
    "min_rcv_lat = np.min(disp_curves[\"receiver_lat\"])\n",
    "min_src_lat = np.min(disp_curves[\"source_lat\"])\n",
    "min_lat = np.min([min_rcv_lat, min_src_lat]) - tol\n",
    "# ---------------------------------------------- #\n",
    "\n",
    "# Get the largest longitude\n",
    "# ---------------------------------------------- #\n",
    "max_rcv_lon = np.max(disp_curves[\"receiver_lon\"])\n",
    "max_src_lon = np.max(disp_curves[\"source_lon\"])\n",
    "max_lon = np.max([max_rcv_lon, max_src_lon])\n",
    "# ---------------------------------------------- #\n",
    "\n",
    "# Get the largest latitude\n",
    "# ---------------------------------------------- #\n",
    "max_rcv_lat = np.max(disp_curves[\"receiver_lat\"])\n",
    "max_src_lat = np.max(disp_curves[\"source_lat\"])\n",
    "max_lat = np.max([max_rcv_lat, max_src_lat])\n",
    "# ---------------------------------------------- #\n",
    "\n",
    "nlon = np.ceil((max_lon + tol - min_lon) / lonstep)\n",
    "nlat = np.ceil((max_lat + tol - min_lat) / latstep)\n",
    "\n",
    "print(f\"Number of nodes in longitude direction: {int(nlon):d}\")\n",
    "print(f\"Number of nodes in latitude direction: {int(nlat):d}\")\n",
    "\n",
    "# Create a grid object, from pysismo\n",
    "grid = Grid(min_lon, lonstep, nlon, min_lat, latstep, nlat)\n",
    "# --------------------------------------------------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763410bf",
   "metadata": {},
   "source": [
    "Here we will calculate the paths. In this case we are simply assuming that the seismic waves propagate along great-circle paths between stations. This assumption may not be very good in areas with large velocity anomalies, but it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df9ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from seismic-noise-tomgraphy\n",
    "\n",
    "# Here I am just appending to a list. This is not necessarily fast, but I'm only appending a few values\n",
    "# so it's ok here. Usually it's better to predefine the array.\n",
    "paths = []\n",
    "dists = []\n",
    "\n",
    "coords = []\n",
    "\n",
    "for idx, row in disp_curves.iterrows():\n",
    "    dist = psutils.dist(row.source_lon, row.source_lat, row.receiver_lon, row.receiver_lat) # calculate the\n",
    "                                                                                            # distance between\n",
    "                                                                                            # points\n",
    "    npts = np.max([np.ceil(dist) + 1, 100])\n",
    "    source_coords = (row.source_lon, row.source_lat)\n",
    "    if source_coords not in coords:\n",
    "        coords.append(source_coords)\n",
    "    receiver_coords = (row.receiver_lon, row.receiver_lat)\n",
    "    if receiver_coords not in coords:\n",
    "        coords.append(receiver_coords)\n",
    "    path = psutils.geodesic(source_coords, receiver_coords, npts) # Calculate the path between source and receiver\n",
    "    \n",
    "    paths.append(path)\n",
    "    dists.append(dist)\n",
    "    \n",
    "dists = np.array(dists)\n",
    "paths = np.array(paths, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f067a",
   "metadata": {},
   "source": [
    "Shown below is the station geometry with raypaths. We can see that the raypath density is very low in the north-west corner of the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 6))\n",
    "for path in paths:\n",
    "    plt.plot(path.T[0], path.T[1], color=\"black\", alpha=0.2)\n",
    "for coord in coords:\n",
    "    plt.plot(coord[0], coord[1], '^', color=\"red\", markersize=10)\n",
    "\n",
    "# Create custom legend\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "my_legend = [Line2D([0], [0], color=\"black\", label=\"raypath\"),\n",
    "             Line2D([0], [0], marker=\"^\", color=\"red\", label=\"Station\", lw=0, markersize=10)]\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "ax.legend(handles=my_legend)\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "ax.set_title(\"Station Geometry\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13470da1",
   "metadata": {},
   "source": [
    "# The Inversion\n",
    "The basic premise of inversion is that the observed data is the matrix multiplacation of the weight matrix 'G', with the model matrix 'm', i.e. $\\textbf{d} = \\textbf{Gm}$. Ideally we could calculate the inverse of the weigh matrix, such that $\\textbf{G}^{-1}\\textbf{d} = \\textbf{m}$. However, reality may be more complicated. In many cases G may be singular, or too large to invert, so more sophisticated approaches are often required.\n",
    "\n",
    "In this scheme, $\\textbf{G}$ is an M x N matrix, where M is the number of raypaths, and N is the total number of nodes in the grid. The values of $\\textbf{G}$ represent the integration of slowness (1 / velocity) through each grid node. For each raypath, the weights in $\\textbf{G}$ will be non-zero for the nodes that the ray samples, while the weights in $\\textbf{G}$ will be zero if the raypath does not sample them. This tends to lead to a very sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from seismic-noise-tomgraphy\n",
    "\n",
    "vels = disp_curves[\"phase_velocity\"]\n",
    "s = (dists / vels).sum() / dists.sum()\n",
    "v0 = 1.0 / s\n",
    "G = rosesutils.make_G(paths, grid, v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c85dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from seismic-noise-tomgraphy\n",
    "\n",
    "dobs = np.matrix(dists / vels - dists / v0).T\n",
    "density = rosesutils.path_density(grid, paths, window=(lonstep, latstep)) # Calculate raypath density\n",
    "path_density = grid.to_2D_array(density) # Reshape the density into an M x N matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e3bea8",
   "metadata": {},
   "source": [
    "This tomgraphy scheme requires the estimation of the covariance matrix of the errors in the observed data. In this case I simply estimate the values of this matrix to be 3 times the standard deviation of the observed velocities. This is probably not the best way of doing this, but it seems to work for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from seismic-noise-tomgraphy\n",
    "\n",
    "sigmav = np.ones((len(vels))) * 3 * np.std(vels) # Set sigma to be 3 time the standard deviation of velocities\n",
    "sigmad = sigmav * dists / vels**2\n",
    "Cinv = np.matrix(np.zeros((len(sigmav), len(sigmav))))\n",
    "np.fill_diagonal(Cinv, 1.0 / sigmad**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aac55dc",
   "metadata": {},
   "source": [
    "In the below plot we can see that the raypath density is very similar to the raypath coverage. Note that the direction of raypaths is also important. If all the raypaths are crossing a grid node in the same direction, then there will be strong smearing in the direction of the ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "extent = (grid.xmin, grid.get_xmax(), grid.ymin, grid.get_ymax())\n",
    "im = ax.imshow(path_density.transpose(), extent=extent, origin=\"lower\")\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label(\"# of Paths\")\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "ax.set_title(\"Path Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38b148",
   "metadata": {},
   "source": [
    "In this section the inversion parameters are setup, and the actual matrix algebra is performed. Some trial and error is often required to find the correct values for these parameters. Try playing around with each of the parameters to get an idea of how this affects the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f845ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from seismic-noise-tomgraphy\n",
    "\n",
    "# Setup the inversion parameters\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "correlation_length = 100 # correlation length a.k.a smoothing width\n",
    "alpha = 15 # Smoothing parameter\n",
    "beta = 1 # Strength of penalization term\n",
    "lambda_ = 0.15 # Sharpness of raypath density weighting function\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "dists_mat = np.zeros((grid.n_nodes(), grid.n_nodes()))\n",
    "i_upper, j_upper = np.triu_indices_from(dists_mat)\n",
    "lons_i, lats_i = grid.xy(i_upper)\n",
    "lons_j, lats_j = grid.xy(j_upper)\n",
    "\n",
    "dists_mat[i_upper, j_upper] = psutils.dist(lons1=lons_i, lats1=lats_i,\n",
    "                                       lons2=lons_j, lats2=lats_j)\n",
    "dists_mat += dists_mat.T\n",
    "\n",
    "# Calculate the smoothing kernel\n",
    "S = np.exp(- dists_mat**2 / (2 * correlation_length**2))\n",
    "S /= S.sum(axis=-1) - np.diag(S)  # normalization of non-diagonal terms\n",
    "\n",
    "# setting up spatial regularization matrix F\n",
    "F = np.matrix(-S)\n",
    "F[np.diag_indices_from(F)] = 1\n",
    "F *= alpha\n",
    "\n",
    "# Calculate regularization matrix Q\n",
    "Q = F.T * F\n",
    "\n",
    "for i, path_density in enumerate(density):\n",
    "    Q[i, i] += beta ** 2 * np.exp(-2 * lambda_ * path_density)\n",
    "    \n",
    "covmopt = np.linalg.inv(G.T * Cinv * G + Q)\n",
    "Ginv = covmopt * G.T # Compute the inverse matrix needed to invert\n",
    "mopt = Ginv * Cinv * dobs # Compute the final estimated model\n",
    "R = Ginv * Cinv * G # Compute the resolution matrix\n",
    "v = grid.to_2D_array(v0 / (1 + mopt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e2cf2",
   "metadata": {},
   "source": [
    "For convenience, I've included a wrapper function around all of the code we've previously seen. So, one can simply call `invert4model` with the appropriate parameters to perform the inversion. Here we'll do just that, for phase velocity measurements at 10s period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26694ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_curves = pd.read_csv(\"data/cam/10.dat\", index_col=None, header=None, delim_whitespace=True)\n",
    "disp_curves.columns = [\"source_name\", \"receiver_name\", \"group_velocity\", \"phase_velocity\",\n",
    "                       \"signal2noise_ratio\", \"source_lon\", \"source_lat\", \"receiver_lon\", \"receiver_lat\",\n",
    "                       \"inter-station_distance\"]\n",
    "v_type = \"phase_velocity\"\n",
    "print(len(disp_curves))\n",
    "std_crit = (np.abs(disp_curves[v_type] - np.mean(disp_curves[v_type]))\\\n",
    "            < 3 * np.std(disp_curves[v_type]))\n",
    "disp_curves = disp_curves[std_crit]\n",
    "print(len(disp_curves))\n",
    "\n",
    "# Set up the inversion parameters\n",
    "# ------------------------------ #\n",
    "alpha = 15\n",
    "beta = 1\n",
    "correlation_length = 30\n",
    "latstep = 0.25\n",
    "lonstep = 0.25\n",
    "# ------------------------------ #\n",
    "\n",
    "# Do the inversion\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "vels, paths, dists = make_paths(disp_curves, v_type)\n",
    "grid = make_grid(disp_curves, tol, latstep, lonstep)\n",
    "v, path_density, R, grid, Cinv, Ginv = invert4model(alpha, beta, lambda_, correlation_length, lonstep,\n",
    "                                                    latstep, grid, vels, dists, paths)\n",
    "# --------------------------------------------------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inset_region = [-30, 60, -40, 40]\n",
    "fine_num_lats = 250\n",
    "fine_num_lons = 250\n",
    "\n",
    "plot_interpolated(grid, v, fine_num_lats, fine_num_lons, path_density, inset_region, \"Phase Velocity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b43427",
   "metadata": {},
   "source": [
    "The above figure shows the tomography result for phase velocity at ten seconds period. We can see the high velocity associated with the Congo Craton in the south-east corner of the map. The general pattern is similar to what is observed in the following figure, adapated from Ojo, Ni & Li, (2017).\n",
    "\n",
    "| Previous Results |  Known Geology |\n",
    "|:---:|:---:|\n",
    "|<img src=\"figures/Ojoetal2017ph_vel.png\" width=\"275\" height=\"200\"> |  <img src=\"figures/known_geology.png\" width=\"300\" height=\"300\">|\n",
    "\n",
    "\n",
    "This is also consistent with the known geology of the area. We know that the Congo Craton is in the south-east, with the Cameroon Volcanic Line to the north-west. We would expect to observe lower velocities associated with the Cameroon Volcanic Line, which is what we see in the tomography map. It is always nice to be able to compare our tomography results with known geology.\n",
    "\n",
    "In order to test the resolution of our model, we will calculate some synthetic observations from a prescribed velocity model, and see if we can invert for the prescribed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8bb1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from seismic-noise-tomography\n",
    "\n",
    "# Set up the inversion parameters\n",
    "# ------------------------------ #\n",
    "alpha = 1\n",
    "beta = 0.5\n",
    "correlation_length = 15\n",
    "latstep = 0.25\n",
    "lonstep = 0.25\n",
    "# ------------------------------ #\n",
    "\n",
    "# The purpose of doing the inversion here is simply to extract the Ginv and Cinv matrices, we will not use\n",
    "# the actual data for the synthetic test\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "vels, paths, dists = make_paths(disp_curves, v_type)\n",
    "grid = make_grid(disp_curves, tol, latstep, lonstep)\n",
    "dobs = np.matrix(dists / vels - dists / v0).T\n",
    "v, path_density, R, grid, Cinv, Ginv = invert4model(alpha, beta, lambda_, correlation_length, lonstep,\n",
    "                                                    latstep, grid, vels, dists, paths)\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "vmid = v0\n",
    "vmin = vmid - 0.5\n",
    "vmax = vmid + 0.5\n",
    "squaresize = 100\n",
    "\n",
    "f_checkerboard = rosesutils.checkerboard_func(grid, vmid, vmin, vmax, squaresize)\n",
    "\n",
    "dsynth = np.zeros_like(dobs)\n",
    "\n",
    "for d, path, curve in zip(dsynth, paths, dists):\n",
    "    lons, lats = path[:, 0], path[:, 1]\n",
    "    \n",
    "    ds = psutils.dist(lons1=lons[:-1], lats1=lats[:-1],\n",
    "                      lons2=lons[1:], lats2=lats[1:])\n",
    "    \n",
    "    v = f_checkerboard(lons, lats)\n",
    "    \n",
    "    t = np.sum(ds * 0.5 * (1.0 / v[:-1] + 1.0 / v[1:]))\n",
    "    \n",
    "    d[...] = t - curve / vmid\n",
    "\n",
    "# Invert for the estimated model\n",
    "msynth = Ginv * Cinv * dsynth\n",
    "mest = grid.to_2D_array(vmid / (1 + msynth))\n",
    "\n",
    "# Get the true model in a form that is ready to plot.\n",
    "lons, lats = grid.xy_nodes()\n",
    "a = grid.to_2D_array(f_checkerboard(lons, lats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38552b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax, ymin, ymax = grid.bbox()\n",
    "\n",
    "lats = np.linspace(ymin, ymax, fine_num_lats)\n",
    "lons = np.linspace(xmin, xmax, fine_num_lons)\n",
    "\n",
    "x = np.digitize(lats, grid.yarray(), right=True)\n",
    "y = np.digitize(lons, grid.xarray(), right=True)\n",
    "\n",
    "fv_est = scipy.interpolate.interp2d(grid.yarray(), grid.xarray(), mest, kind=\"cubic\")\n",
    "fv_true = scipy.interpolate.interp2d(grid.yarray(), grid.xarray(), a, kind=\"cubic\")\n",
    "\n",
    "v_interp_est = fv_est(lats, lons)\n",
    "v_interp_true = fv_true(lats, lons)\n",
    "\n",
    "# Mask areas with no raypaths\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(y)):\n",
    "        dens = path_density[y[j], x[i]]\n",
    "        if dens < 1.0:\n",
    "            v_interp_est[j, i] = v_interp_est[j, i] * np.nan\n",
    "            v_interp_true[j, i] = v_interp_true[j, i] * np.nan\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "grd_est = xr.DataArray(v_interp_est.T, coords=(lats, lons)) # Get the data in a format that pygmt can use\n",
    "grd_true = xr.DataArray(v_interp_true.T, coords=(lats, lons))\n",
    "\n",
    "fig = pygmt.Figure()\n",
    "with fig.subplot(\n",
    "    nrows=1,\n",
    "    ncols=2,\n",
    "    figsize=(\"15c\", \"12c\"),  # width of 15 cm, height of 6 cm\n",
    "    autolabel=False,\n",
    "    margins=[\"0.8c\", \"0.0c\"],  # horizontal 0.3 cm and vertical 0.2 cm margins\n",
    "    title=\"Synthetic Test Results\",\n",
    "    sharex=\"b\",  # shared x-axis on the bottom side\n",
    "    sharey=\"l\",  # shared y-axis on the left side\n",
    "    frame=True\n",
    "    \n",
    "):\n",
    "    with fig.set_panel(0):\n",
    "        fig.coast(\n",
    "            region=f\"{xmin-1}/{xmax+1}/{ymin-1}/{ymax+1}\",\n",
    "            land=\"lightgray\", # Color the land light gray\n",
    "            water=\"white\", # color the water white\n",
    "            borders=1, # Plot national boundaries\n",
    "            shorelines=True, # Show shorelines\n",
    "        )\n",
    "        \n",
    "        # Make a colormap\n",
    "        pygmt.makecpt(\n",
    "              cmap=\"inferno\", reverse=True,\n",
    "              series=[np.nanmin(a), np.nanmax(a)]\n",
    "        )\n",
    "        \n",
    "        # Show the tomography data\n",
    "        fig.grdimage(\n",
    "                     grd_est,\n",
    "                     frame=True,\n",
    "                     cmap=True,\n",
    "                     nan_transparent=True,\n",
    "                     transparency=20\n",
    "        )\n",
    "        \n",
    "        # Label the frames. Not the cleanest solution but it works.\n",
    "        fig.text(\n",
    "            x=12.5,\n",
    "            y=-0.5,\n",
    "            text=\"Recovered Model\",\n",
    "            font=\"14p\",\n",
    "            no_clip=True\n",
    "        )        \n",
    "    with fig.set_panel(1):\n",
    "        \n",
    "        fig.coast(\n",
    "            region=f\"{xmin-1}/{xmax+1}/{ymin-1}/{ymax+1}\",\n",
    "            land=\"lightgray\", # Color the land light gray\n",
    "            water=\"white\", # color the water white\n",
    "            borders=1, # Plot national boundaries\n",
    "            shorelines=True, # Show shorelines\n",
    "        )\n",
    "        \n",
    "        # Make a colormap\n",
    "        pygmt.makecpt(\n",
    "              cmap=\"inferno\", reverse=True,\n",
    "              series=[np.nanmin(a), np.nanmax(a)]\n",
    "        )\n",
    "        \n",
    "        # Show the tomography data\n",
    "        fig.grdimage(\n",
    "                     grd_true,\n",
    "                     frame=True,\n",
    "                     cmap=True,\n",
    "                     nan_transparent=True,\n",
    "                     transparency=20\n",
    "        )\n",
    "        \n",
    "        # Label the frames. Not the cleanest solution but it works.\n",
    "        fig.text(\n",
    "            x=12.5, # Place text at 12.5 degrees east\n",
    "            y=-0.5, # Place text at 0.5 degrees south\n",
    "            text=\"True Model\",\n",
    "            font=\"14p\",\n",
    "            no_clip=True\n",
    "        )\n",
    "    fig.colorbar(frame='x+l\"Phase Velocity [km/s]\"', position=\"+o-4.5c/-3c+w7c/0.5c+h\")   \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482058c",
   "metadata": {},
   "source": [
    "In the synthetic test, we can see that checkers with a point to point seperation of 100km are well resolved. This is a good sign that the results we have seen in the real inversion are reasonable.\n",
    "\n",
    "**Task:** Try playing around with the synthetic inversion on your own! What is the smallest square size that you can resolve?\n",
    "\n",
    "\n",
    "There we go, that's the end of the lab! We've created a 2-D map of surface wave velocities at one period. While a lot of shortcuts were taken to produce this image, we can still see some features that correspond to known geology. Notably, the effect of the Congo Craton is observed as a high velocity anomaly in the south-east corner of the map. In order to make further inferences about the geology, it would be useful to create a 3-D shear wave velocity model. This is generally done by extracting the velocities at each period for one geographic location. The 1-D dispersion curve that results from this can then be inverted for the shear wave structure at that location. Doing this for each geographic location in the study grid produces a pseudo 3-D model, with a 1-D shear wave model at each grid node. I'll save this problem for another day. Here are some codes that may be used for this purpose.\n",
    "\n",
    "- https://github.com/jenndrei/BayHunter\n",
    "- http://www.iearth.org.au/codes/NA/\n",
    "\n",
    "Note that there is another dataset in the data directory. This dataset contains data for the Georgia Basin in British Columbia. Try making a 2-D period map from it with your new skills!\n",
    "\n",
    "# References\n",
    "\n",
    "- Barmin, M. P., Ritzwoller, M. H., & Levshin, A. L. (2001). A fast and reliable method for surface wave tomography. In Monitoring the comprehensive nuclear-test-ban treaty: Surface waves (pp. 1351-1375). _Birkhäuser, Basel_. https://doi.org/10.1007/PL00001225\n",
    "- Goutorbe, B., de Oliveira Coelho, D. L., & Drouet, S. (2015). Rayleigh wave group velocities at periods of 6–23 s across Brazil from ambient noise tomography. _Geophysical Journal International_, 203(2), 869-882. https://doi.org/10.1093/gji/ggv343\n",
    "- Jiang, C., & Denolle, M. A. (2020). NoisePy: A new high‐performance python tool for ambient‐noise seismology. _Seismological Research Letters_, 91(3), 1853-1866. https://doi.org/10.1785/0220190364\n",
    "- Ojo, A. O., Ni, S., & Li, Z. (2017). Crustal radial anisotropy beneath Cameroon from ambient noise tomography. _Tectonophysics_, 696, 37-51. https://doi.org/10.1016/j.tecto.2016.12.018\n",
    "\n",
    "# Useful Codes\n",
    "\n",
    "- https://github.com/chengxinjiang/NoisePy\n",
    "- https://github.com/bgoutorbe/seismic-noise-tomography\n",
    "- https://github.com/jxensing/ftanpy\n",
    "- https://github.com/ThomasLecocq/msnoise-tomo\n",
    "- http://rses.anu.edu.au/~nick/surftomo.html\n",
    "- https://github.com/NoiseCIEI/AFTAN\n",
    "- https://github.com/HongjianFang/DSurfTomo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roses_2021",
   "language": "python",
   "name": "roses_2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
